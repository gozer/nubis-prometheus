ALERT InstanceDown
  IF up{job="node"} * ON(instance) GROUP_LEFT(project, environment, account) nubis * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws == 0
  FOR 10m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Instance {{ $labels.instance }} down",
    description = "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 10 minutes.",
  }

ALERT ConsulDown
  IF drop_common_labels(label_replace(label_replace(sum(consul_agent_check{check="serfHealth",job="consul"} == 1) BY (node), "instance", "$1", "node", "(.*)"),"node","XXX","node",".*")) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws != 3
  FOR 5m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Consul not healthy on {{ $labels.node }}",
    description = "{{ $labels.node }} Consul se health reporting unhealthy",
  }

# We expect all our Consul services to have at least 1 healthy member
# Except for tainted instances, these we filter out here
ALERT ConsulServiceDown
  IF drop_common_labels(label_replace(label_replace(min(consul_catalog_service_node_healthy{service!="tainted"}) BY (service, node)  - ON  (node) GROUP_LEFT(instance) min(consul_agent_check{check="serfHealth"}) BY (node), "instance", "$1", "node", "(.*)"), "node", "XXX", "node", ".*")) * ON(instance) GROUP_LEFT(project, environment, account) nubis * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws < 0
  FOR 5m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Consul service problem for {{ $labels.service }}",
    description = "Consul detected {{ $labels.service }} with no healthy nodes",
  }

# Create an alert for tainted istances past their expriry date

ALERT SustainedLoadHigh
  IF avg_over_time(node_load1[5m]) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws > 8
  FOR 15m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Sustained HIGH load on {{ $labels.instance }}",
    description = "{{ $labels.instance }} average 1 minute load {{ $value }} is averaging over 8 for more than 15 minutes",
  }

ALERT OutlierLoad1
  IF up * node_load5 * ON(instance) GROUP_LEFT(project, environment, account) nubis > 0.25 > ON(project) GROUP_LEFT avg(up * node_load5 * ON(instance) GROUP_LEFT(project, environment, account) nubis) by (project) + ( 2*stddev(up * node_load5 * ON(instance) GROUP_LEFT(project, environment, account) nubis) by (project) )
  FOR 30m
  LABELS {
    severity = "critical",
    platform = "nubis",
    type = "anomaly"
  }
  ANNOTATIONS {
    summary = "Anomalous load on {{ $labels.instance }}",
    description = "{{ $labels.instance }} is showing load=={{ $value }} that's more than 2 stddevs over the average"
  }

ALERT ConsulInsufficientPeers
  IF count(up{job="consul"} == 0) > (count(up{job="consul"}) / 2 - 1)
  FOR 5m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Consul Insufficient Peers",
    description = "The consul has lost sufficient peers and will be unable to achieve Quorum"
  }

ALERT ConsulProxyAvailable
  IF absent(count(count(consul_catalog_service_node_healthy{job="consul",service="proxy"} == 1) by (node)) == 2)
  FOR 5m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Unexpected number of healthy proxy nodes",
    description = "Healthy Proxy servers count isn't 2"
  }

ALERT ConsulConsulAvailable
  IF absent(count(count(consul_catalog_service_node_healthy{job="consul",service="consul"} == 1) by (node)) == 3)
  FOR 5m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Unexpected number of healthy Consul nodes",
    description = "Healthy Consul servers count isn't 3"
  }

ALERT ConsulFluentAvailable
  IF absent(count(count(consul_catalog_service_node_healthy{job="consul",service="fluentd"} == 1) by (node)) == 1)
  FOR 5m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Unexpected number of healthy Fluentd nodes",
    description = "Healthy fluentd servers count isn't 1"
  }

ALERT ConsulPrometheusAvailable
  IF absent(count(count(consul_catalog_service_node_healthy{job="consul",service="prometheus"} == 1) by (node)) == 1)
  FOR 5m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Unexpected number of healthy Fluentd nodes",
    description = "Healthy prometheus servers count isn't 1"
  }

ALERT ConsulAlertmanagerAvailable
  IF absent(count(count(consul_catalog_service_node_healthy{job="consul",service="alertmanager"} == 1) by (node)) == 1)
  FOR 5m
  LABELS {
    severity = "critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "Unexpected number of healthy Fluentd nodes",
    description = "Healthy alertmanager servers count isn't 1"
  }

ALERT PredictDiskFull24h
  IF predict_linear(node_filesystem_free{job='node'}[1h], 24*60*60) * ON(instance) GROUP_LEFT(project, environment, account) nubis * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws < 0
  FOR 30m
  LABELS {
    severity="critical",
    platform = "nubis",
    type="predictive"
  }
  ANNOTATIONS {
    summary = "Disk is going to be full in < 24h",
    description = "The disk {{ $labels.mountpoint }} on {{ $labels.instance }} is filling up too fast, full in 24 hours or less"
  }

ALERT FluentdBufferSize
  IF abs(avg_over_time(fluentd_status_buffer_total_bytes[5m]) - avg_over_time(fluentd_status_buffer_total_bytes[60m])) > 1 * stddev_over_time(fluentd_status_buffer_total_bytes[60m]) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws
  FOR 5m
  LABELS {
    severity="critical",
    platform = "nubis",
    type="anomaly"
  }
  ANNOTATIONS {
    summary = "Fluentd queue size increasing too fast",
    description = "The fluentd queue size on {{ $labels.instance }} for {{ $labels.type }} has been growing too fast"
  }

ALERT FluentdQueueLength
  IF abs(avg_over_time(fluentd_status_buffer_queue_length[5m]) - avg_over_time(fluentd_status_buffer_queue_length[60m])) > 1 * stddev_over_time(fluentd_status_buffer_queue_length[60m]) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws
  FOR 5m
  LABELS {
    severity="critical",
    platform = "nubis",
    type="anomaly"
  }
  ANNOTATIONS {
    summary = "Fluentd queue length increasing too fast",
    description = "The fluentd queue length on {{ $labels.instance }} for {{ $labels.type }} has been growing too fast"
  }

ALERT FluentdRetryRate
  IF irate(fluentd_status_retry_count[5m]) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws > 0
  FOR 5m
  LABELS {
    severity="critical",
    platform = "nubis",
    type="anomaly"
  }
  ANNOTATIONS {
    summary = "Fluentd has been retrying to push out",
    description = "The fluentd plugin for {{ $labels.type }} on {{ $labels.instance }} has been forced to retry {{ $value }} times/s recently, possible issue with push destination"
  }

ALERT NATNetConntrackFull
  IF 100 * node_nf_conntrack_entries / node_nf_conntrack_entries_limit * ON(instance) GROUP_LEFT(project, environment, account) nubis * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws > 90
  FOR 15m
  LABELS {
    severity="critical",
    platform = "nubis"
  }
  ANNOTATIONS {
    summary = "IP Connection Tracking Almost Full",
    description = "IP Connection tracking table almost full ({{ $value }}>90%) on {{ $labes.instance }}"
  }

ALERT NATIpForwardingNotEnabled
  IF node_netstat_Ip_Forwarding * ON(instance) GROUP_LEFT(project, environment, account) nubis{project="nat"} * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws != 1
  FOR 5m
  LABELS {
    severity="critical",
    platform = "nubis",
    type="invariant"
  }
  ANNOTATIONS {
    summary = "IP Forwarding not enabled on NAT node",
    description = "IP Fowarding is not enabled on NAT node {{ $labels.instance }}"
  }

ALERT IpForwardingEnabledNonNAT
  IF node_netstat_Ip_Forwarding * ON(instance) GROUP_LEFT(project, environment, account) nubis{project!="nat"} * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws != 2
  FOR 5m
  LABELS {
    severity="critical",
    platform = "nubis",
    type="invariant"
  }
  ANNOTATIONS {
    summary = "IP Forwarding enabled on non-NAT node",
    description = "IP Fowarding is enabled on non-NAT node {{ $labels.instance }}"
  }

ALERT CronJobFailing
  IF count(nubis_cron_status != 0) by(cronjob) !=0
  FOR 15m
  LABELS {
    severity="critical",
    platform = "nubis",
    type="failure"
  }
  ANNOTATIONS {
    summary = "Cronjob {{ $labels.cronjob }} has been failing",
    description = "Instances of a cronjob called {{ $labels.cronjob }} has failed {{ $value }} times in the last 15 minutes"
  }
