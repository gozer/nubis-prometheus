ALERT InstanceDown
  IF up{job="node"} * ON(instance) GROUP_LEFT(project, environment, account) nubis * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws == 0
  FOR 1m
  LABELS { severity = "critical" }
  ANNOTATIONS {
    summary = "Instance {{ $labels.instance }} down",
    description = "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.",
  }

ALERT ConsulDown
  IF drop_common_labels(label_replace(label_replace(sum(consul_agent_check{check="serfHealth",job="consul"} == 1) BY (node), "instance", "$1", "node", "(.*)"),"node","XXX","node",".*")) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws != 3
  FOR 5m
  LABELS { severity = "critical" }
  ANNOTATIONS {
    summary = "Consul not healthy on {{ $labels.node }}",
    description = "{{ $labels.node }} Consul se health reporting unhealthy",
  }

# We expect all our Consul services to have at least 1 healthy member
# Except for tainted instances, these we filter out here
ALERT ConsulServiceDown
  IF drop_common_labels(label_replace(label_replace(min(consul_catalog_service_node_healthy{service!="tainted"}) BY (service,node), "instance", "$1", "node", "(.*)"), "node", "XXX", "node", ".*")) * ON(instance) GROUP_LEFT(project, environment, account) nubis * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws < 1
  FOR 5m
  LABELS { severity = "critical" }
  ANNOTATIONS {
    summary = "Consul service problem for {{ $labels.service }}",
    description = "Consul detected {{ $labels.service }} with no healthy nodes",
  }

# Create an alert for tainted istances past their expriry date

ALERT SustainedLoadHigh
  IF avg_over_time(node_load1[5m]) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws > 2
  FOR 10m
  LABELS { severity = "critical" }
  ANNOTATIONS {
    summary = "Sustained HIGH load on {{ $labels.instance }}",
    description = "{{ $labels.instance }} average 1 minute load is averaging over 2 for more than 10 minutes",
  }

ALERT OutlierLoad1
  IF (node_load1 > 0.25 > ON(job) GROUP_LEFT(account) (avg(node_load1) BY (job)  + ON(job) 2 * stddev(node_load1) BY (job))) * ON(instance) GROUP_LEFT(project, environment, account) nubis * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws
  FOR 5m
  LABELS {
    severity = "critical",
    type = "anomaly"
  }
  ANNOTATIONS {
    summary = "Anomalous load on {{ $labels.instance }}",
    description = "{{ $labels.instance }} is showing load=={{ $value }} that's more than 2 stddevs over the average"
  }

ALERT ConsulInsufficientPeers
  IF count(up{job="consul"} == 0) > (count(up{job="consul"}) / 2 - 1)
  FOR 5m
  LABELS { severity = "critical" }
  ANNOTATIONS {
    summary = "Consul Insufficient Peers",
    description = "The consul has lost sufficient peers and will be unable to achieve Quorum"
  }

ALERT ConsulProxyAvailable
  IF count(count(consul_catalog_service_node_healthy{job="consul",service="proxy"} == 1) by (node)) != 2
  FOR 5m
  LABELS { severity = "critical" }
  ANNOTATIONS {
    summary = "Unexpected number of healthy proxy nodes",
    description = "There are {{ $value }} healthy proxies and not the expected 2"

ALERT ConsulConsulAvailable
  IF count(count(consul_catalog_service_node_healthy{job="consul",service="consul"} == 1) by (node)) != 3
  FOR 5m
  LABELS { severity = "critical" }
  ANNOTATIONS {
    summary = "Unexpected number of healthy Consul nodes",
    description = "There are {{ $value }} healthy Consul servers and not the expected 3"
  }

ALERT ConsulFluentAvailable
  IF count(count(consul_catalog_service_node_healthy{job="consul",service="fluentd"} == 1) by (node)) != 1
  FOR 5m
  LABELS { severity = "critical" }
  ANNOTATIONS {
    summary = "Unexpected number of healthy Fluentd nodes",
    description = "There are {{ $value }} healthy fluentd servers and not the expected 1"
  }

ALERT PredictDiskFull24h
  IF predict_linear(node_filesystem_free{job='node'}[1h], 24*60*60) * ON(instance) GROUP_LEFT(project, environment, account) nubis * ON(instance) GROUP_LEFT(instance_type, availability_zone, region) aws < 0
  FOR 5m
  LABELS {
    severity="critical",
    type="predictive"
  }
  ANNOTATIONS {
    summary = "Disk is going to be full in < 24h",
    description = "The disk {{ $labels.mountpoint }} on {{ $labels.instance }} is filling up too fast, full in 24 hours or less"
  }

ALERT FluentdBufferSize
  IF abs(avg_over_time(fluentd_status_buffer_total_bytes[5m]) - avg_over_time(fluentd_status_buffer_total_bytes[60m])) > 1 * stddev_over_time(fluentd_status_buffer_total_bytes[60m]) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws
  FOR 5m
  LABELS {
    severity="critical",
    type="anomaly"
  }
  ANNOTATIONS {
    summary = "Fluentd queue size increasing too fast",
    description = "The fluentd queue size on {{ $labels.instance }} for {{ $labels.type }} has been growing too fast"
  }

ALERT FluentdQueueLength
  IF abs(avg_over_time(fluentd_status_buffer_queue_length[5m]) - avg_over_time(fluentd_status_buffer_queue_length[60m])) > 1 * stddev_over_time(fluentd_status_buffer_queue_length[60m]) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws
  FOR 5m
  LABELS {
    severity="critical",
    type="anomaly"
  }
  ANNOTATIONS {
    summary = "Fluentd queue length increasing too fast",
    description = "The fluentd queue length on {{ $labels.instance }} for {{ $labels.type }} has been growing too fast"
  }

ALERT FluentdRetryRate
  IF irate(fluentd_status_retry_count[5m]) * on (instance) group_left (project,environment,account) nubis  * on (instance) group_left(instance_type, availability_zone, region) aws > 0
  FOR 5m
  LABELS {
    severity="critical",
    type="anomaly"
  }
  ANNOTATIONS {
    summary = "Fluentd has been retrying to push out",
    description = "The fluentd plugin for {{ $labels.type }} on {{ $labels.instance }} has been forced to retry {{ $value }} times/s recently, possible issue with push destination"
  }
